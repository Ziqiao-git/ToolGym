#!/usr/bin/env python3
"""
Emergency Test Analysis - Tool Failure Recovery Analysis

Analyzes emergency test results comparing three strategies:
1. no_interception: Baseline (no failures)
2. first_non_search: First non-search tool fails once
3. random_20: 20% random failure rate

Two main analyses:
1. Final Answer Quality: Use CommonLLMJudge with 3 judges, compare score drops
2. Recovery Capability: Analyze how models recover from intercepted errors

Usage:
    # 1. Quick recovery analysis (no evaluation needed)
    python analyze_scripts/analyze_emergency.py --recovery-only

    # 2. Run full evaluation + analysis (~10 minutes)
    python analyze_scripts/analyze_emergency.py --evaluate

    # 3. Show scores from existing evaluations + recovery
    python analyze_scripts/analyze_emergency.py --show-scores

    # 4. Custom options
    python analyze_scripts/analyze_emergency.py --evaluate \\
        --model claude-3.5-sonnet \\
        --pass-number 1 \\
        --judges "openai/gpt-4o-mini,deepseek/deepseek-chat,openai/gpt-5.1-chat"

Data Requirements:
    - Trajectories: trajectories/Emergency_test/{model}/pass@{N}/{strategy}/*.json
    - Generated by: python runtime/emergency_test.py --strategy all

Output:
    1. Final Answer Score Comparison:
       - Mean/median/stdev scores per strategy
       - Score drop vs baseline (no_interception)
       - Example: "first_non_search: -0.05 (12.5% drop)"

    2. Recovery Capability Analysis:
       - Average recovery score (0-1, from analyze_retry.py methodology)
       - Breakdown of recovery actions:
         * Retry same tool (success): 1.0
         * Switch to different tool (success): 0.8
         * Search for new tools: 0.6
         * Retry/switch but fail: 0.2
         * Give up (no more actions): 0.0

Evaluation Output Location:
    evaluation/Emergency_test/{model}/pass@{N}/{strategy}/{judge}/
"""
from __future__ import annotations

import sys
import os
import json
import argparse
import subprocess
import statistics
from pathlib import Path
from collections import defaultdict
from typing import Dict, Any, List, Optional

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

STRATEGIES = ["no_interception", "first_non_search", "random_20"]

DEFAULT_JUDGES = [
    "openai/gpt-4o-mini",
    "deepseek/deepseek-v3.2",
    "openai/gpt-5.1-chat"
]


def run_commonllm_judge(
    model: str,
    pass_number: int,
    judge_models: List[str],
) -> Dict[str, Dict[str, List[float]]]:
    """
    Run CommonLLMJudge for all strategies with multiple judges.

    Returns:
        Dict[strategy, Dict[query_uuid, List[scores_from_judges]]]
    """
    print("\n" + "=" * 80)
    print("RUNNING COMMONLLMJUDGE EVALUATION")
    print("=" * 80)
    print(f"Judges: {', '.join(judge_models)}\n")

    emergency_dir = PROJECT_ROOT / "trajectories" / "Emergency_test" / model / f"pass@{pass_number}"
    results = {}

    for strategy in STRATEGIES:
        print(f"\n{'='*70}")
        print(f"Strategy: {strategy}")
        print(f"{'='*70}")

        strategy_dir = emergency_dir / strategy
        if not strategy_dir.exists():
            print(f"  ⚠ Directory not found: {strategy_dir}")
            continue

        strategy_scores = defaultdict(list)  # query_uuid -> [score1, score2, score3]

        for judge_model in judge_models:
            print(f"\n  Judge: {judge_model}")

            # Output directory for evaluations
            eval_output_dir = (
                PROJECT_ROOT / "evaluation" / "Emergency_test" / model /
                f"pass@{pass_number}" / strategy / judge_model.replace("/", "-")
            )
            eval_output_dir.mkdir(parents=True, exist_ok=True)

            # Run CommonLLMJudge
            cmd = [
                "python",
                str(PROJECT_ROOT / "Orchestrator/mcpuniverse/evaluator/commonllmjudge.py"),
                "--traj_dir", str(strategy_dir),
                "--model", judge_model,
                "--step-by-step",
                "--recursive",
                "--output-dir", str(eval_output_dir),
                "--parallel", "10",
            ]

            try:
                print(f"    Running evaluation...")
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=600,
                    cwd=str(PROJECT_ROOT)
                )

                if result.returncode != 0:
                    print(f"    ✗ Error: {result.stderr[:200]}")
                    continue

                print(f"    ✓ Completed")

            except subprocess.TimeoutExpired:
                print(f"    ✗ Timeout")
                continue
            except Exception as e:
                print(f"    ✗ Error: {e}")
                continue

            # Read evaluation results from nested directory structure
            # CommonLLMJudge creates: {output_dir}/{strategy}_by_{judge}/pass@{N}/eval_*.json
            eval_files = list(eval_output_dir.glob("**/eval_*.json"))

            for eval_file in eval_files:
                try:
                    with open(eval_file, 'r') as f:
                        eval_data = json.load(f)

                    # Extract query UUID from filename: eval_<uuid>.json
                    filename = eval_file.stem
                    if filename.startswith("eval_"):
                        query_uuid = filename[5:]  # Remove "eval_" prefix
                        overall_score = eval_data.get("overall_score", 0.0)
                        strategy_scores[query_uuid].append(overall_score)

                except Exception as e:
                    print(f"    ⚠ Error reading {eval_file.name}: {e}")

        results[strategy] = dict(strategy_scores)
        print(f"\n  Total evaluated: {len(strategy_scores)} queries")

    return results


def load_evaluation_scores(
    model: str,
    pass_number: int,
    judge_models: List[str],
) -> Dict[str, Dict[str, List[float]]]:
    """
    Load existing evaluation scores from disk.

    Returns:
        Dict[strategy, Dict[query_uuid, List[scores_from_judges]]]
    """
    print("\n" + "=" * 80)
    print("LOADING EXISTING EVALUATION SCORES")
    print("=" * 80 + "\n")

    results = {}

    for strategy in STRATEGIES:
        strategy_scores = defaultdict(list)

        for judge_model in judge_models:
            eval_output_dir = (
                PROJECT_ROOT / "evaluation" / "Emergency_test" / model /
                f"pass@{pass_number}" / strategy / judge_model.replace("/", "-")
            )

            if not eval_output_dir.exists():
                print(f"⚠ Not found: {strategy}/{judge_model.split('/')[-1]}")
                continue

            # Read from nested directory structure
            eval_files = list(eval_output_dir.glob("**/eval_*.json"))

            for eval_file in eval_files:
                try:
                    with open(eval_file, 'r') as f:
                        eval_data = json.load(f)

                    filename = eval_file.stem
                    if filename.startswith("eval_"):
                        query_uuid = filename[5:]  # Remove "eval_" prefix
                        overall_score = eval_data.get("overall_score", 0.0)
                        strategy_scores[query_uuid].append(overall_score)

                except Exception as e:
                    continue

            print(f"✓ Loaded {len(eval_files)} scores: {strategy}/{judge_model.split('/')[-1]}")

        results[strategy] = dict(strategy_scores)

    return results


def print_score_comparison(
    scores: Dict[str, Dict[str, List[float]]],
):
    """
    Print comparison of final answer scores across strategies.

    Args:
        scores: Dict[strategy, Dict[query_uuid, List[scores_from_judges]]]
    """
    print("\n" + "=" * 80)
    print("FINAL ANSWER SCORE COMPARISON")
    print("=" * 80)
    print("\nScores averaged across 3 LLM judges (gpt-4o-mini, deepseek-chat, gpt-5.1-chat)\n")

    # Calculate average scores per query per strategy
    avg_scores_by_strategy = {}

    for strategy in STRATEGIES:
        strategy_data = scores.get(strategy, {})
        avg_scores = {}

        for query_uuid, judge_scores in strategy_data.items():
            if judge_scores:
                avg_scores[query_uuid] = statistics.mean(judge_scores)

        avg_scores_by_strategy[strategy] = avg_scores

    # Calculate aggregate statistics
    stats = {}
    for strategy in STRATEGIES:
        avg_scores = list(avg_scores_by_strategy[strategy].values())

        if avg_scores:
            stats[strategy] = {
                "mean": statistics.mean(avg_scores),
                "median": statistics.median(avg_scores),
                "stdev": statistics.stdev(avg_scores) if len(avg_scores) > 1 else 0.0,
                "count": len(avg_scores),
            }
        else:
            stats[strategy] = {
                "mean": 0.0,
                "median": 0.0,
                "stdev": 0.0,
                "count": 0,
            }

    # Print table
    print(f"{'Strategy':<20} {'Count':<8} {'Mean':<10} {'Median':<10} {'StdDev':<10} {'vs Baseline'}")
    print("-" * 80)

    baseline_mean = stats["no_interception"]["mean"]

    for strategy in STRATEGIES:
        s = stats[strategy]
        count = s["count"]
        mean = s["mean"]
        median = s["median"]
        stdev = s["stdev"]

        # Calculate drop from baseline
        if strategy != "no_interception" and baseline_mean > 0:
            drop = baseline_mean - mean
            drop_pct = (drop / baseline_mean) * 100
            vs_baseline = f"-{drop:.3f} ({drop_pct:.1f}%)"
        else:
            vs_baseline = "baseline"

        print(f"{strategy:<20} {count:<8} {mean:.3f}      {median:.3f}      {stdev:.3f}      {vs_baseline}")

    # Detailed score drop analysis
    if baseline_mean > 0:
        print("\n" + "-" * 80)
        print("SCORE DROP ANALYSIS")
        print("-" * 80)
        print(f"\nBaseline (no_interception): {baseline_mean:.3f}\n")

        for strategy in ["first_non_search", "random_20"]:
            mean = stats[strategy]["mean"]
            drop = baseline_mean - mean
            drop_pct = (drop / baseline_mean) * 100

            print(f"{strategy}:")
            print(f"  Score: {mean:.3f}")
            print(f"  Drop:  {drop:.3f} ({drop_pct:.1f}%)")
            print()


def analyze_recovery(
    model: str,
    pass_number: int,
):
    """
    Analyze recovery capability for intercepted queries.

    Uses analyze_retry.py scoring:
    - Retry same tool + success: 1.0
    - Switch tool + success: 0.8
    - Search new tools: 0.6
    - Retry/switch + fail: 0.2
    - Give up (end with error): 0.0
    """
    print("\n" + "=" * 80)
    print("RECOVERY CAPABILITY ANALYSIS")
    print("=" * 80)
    print("\nAnalyzing recovery from INTERCEPTED tool failures only...\n")

    emergency_dir = PROJECT_ROOT / "trajectories" / "Emergency_test" / model / f"pass@{pass_number}"

    for strategy in ["first_non_search", "random_20"]:
        print(f"\n{'='*70}")
        print(f"Strategy: {strategy}")
        print(f"{'='*70}\n")

        strategy_dir = emergency_dir / strategy
        if not strategy_dir.exists():
            print(f"  Directory not found")
            continue

        traj_files = list(strategy_dir.glob("trajectory_*.json"))

        total_queries = 0
        intercepted_queries = 0
        total_interceptions = 0
        recovery_scores = []
        recovery_actions = defaultdict(int)

        for traj_file in traj_files:
            try:
                with open(traj_file, 'r') as f:
                    traj = json.load(f)

                total_queries += 1

                # Check if intercepted
                stats = traj.get("metadata", {}).get("interception_stats", {})
                if not stats.get("intercepted", False):
                    continue

                intercepted_queries += 1
                interception_count = stats.get("interception_count", 0)
                total_interceptions += interception_count

                # Analyze recovery behavior from reasoning_trace
                reasoning_trace = traj.get("reasoning_trace", [])
                interception_log = stats.get("interception_log", [])
                error_msg = "Tool temporarily unavailable" if not interception_log else interception_log[0].get("error_message", "")

                # Find intercepted calls in reasoning trace
                for i, entry in enumerate(reasoning_trace):
                    if entry.get("type") == "result" and error_msg in entry.get("content", ""):
                        # Found an intercepted tool call, look for the action that was attempted
                        failed_tool = None
                        for j in range(i-1, max(0, i-3), -1):
                            if reasoning_trace[j].get("type") == "action":
                                # Extract tool name from action string
                                action_str = reasoning_trace[j].get("content", "")
                                if "Using tool `" in action_str:
                                    tool_start = action_str.find("`") + 1
                                    tool_end = action_str.find("`", tool_start)
                                    failed_tool = action_str[tool_start:tool_end]
                                    break

                        # Find next action after failure
                        next_tool = None
                        next_success = False

                        for j in range(i+1, min(len(reasoning_trace), i+10)):
                            if reasoning_trace[j].get("type") == "action":
                                action_str = reasoning_trace[j].get("content", "")
                                if "Using tool `" in action_str:
                                    tool_start = action_str.find("`") + 1
                                    tool_end = action_str.find("`", tool_start)
                                    next_tool = action_str[tool_start:tool_end]

                                    # Check if search_tools
                                    if next_tool == "search_tools":
                                        recovery_scores.append(0.6)
                                        recovery_actions["search_tools"] += 1
                                        break

                                    # Check if it succeeded
                                    for k in range(j+1, min(len(reasoning_trace), j+5)):
                                        if reasoning_trace[k].get("type") == "result":
                                            result_content = reasoning_trace[k].get("content", "")
                                            next_success = error_msg not in result_content
                                            break

                                    # Score recovery
                                    if failed_tool == next_tool:
                                        # Retry same
                                        if next_success:
                                            recovery_scores.append(1.0)
                                            recovery_actions["retry_same_success"] += 1
                                        else:
                                            recovery_scores.append(0.2)
                                            recovery_actions["retry_same_fail"] += 1
                                    else:
                                        # Switch tool
                                        if next_success:
                                            recovery_scores.append(0.8)
                                            recovery_actions["switch_tool_success"] += 1
                                        else:
                                            recovery_scores.append(0.2)
                                            recovery_actions["switch_tool_fail"] += 1
                                    break

                        # If no next tool found, gave up
                        if next_tool is None:
                            recovery_scores.append(0.0)
                            recovery_actions["gave_up"] += 1

            except Exception as e:
                print(f"  Error processing {traj_file.name}: {e}")

        # Print results
        print(f"Total queries: {total_queries}")
        print(f"Intercepted queries: {intercepted_queries}")
        print(f"Total interceptions: {total_interceptions}")

        if recovery_scores:
            avg_recovery = statistics.mean(recovery_scores)
            print(f"\nAverage Recovery Score: {avg_recovery:.3f}")

            print(f"\nRecovery Actions:")
            print(f"  Retry same (success): {recovery_actions['retry_same_success']} (score: 1.0)")
            print(f"  Retry same (fail):    {recovery_actions['retry_same_fail']} (score: 0.2)")
            print(f"  Switch tool (success): {recovery_actions['switch_tool_success']} (score: 0.8)")
            print(f"  Switch tool (fail):    {recovery_actions['switch_tool_fail']} (score: 0.2)")
            print(f"  Search tools:          {recovery_actions['search_tools']} (score: 0.6)")
            print(f"  Gave up:               {recovery_actions['gave_up']} (score: 0.0)")
        else:
            print(f"\nNo intercepted queries found")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze emergency test results"
    )
    parser.add_argument(
        "--model",
        default="claude-3.5-sonnet",
        help="Model name (default: claude-3.5-sonnet)"
    )
    parser.add_argument(
        "--pass-number",
        type=int,
        default=1,
        help="Pass number (default: 1)"
    )
    parser.add_argument(
        "--evaluate",
        action="store_true",
        help="Run CommonLLMJudge evaluation (takes ~10 min)"
    )
    parser.add_argument(
        "--show-scores",
        action="store_true",
        help="Show score comparison from existing evaluations"
    )
    parser.add_argument(
        "--judges",
        default=",".join(DEFAULT_JUDGES),
        help=f"Comma-separated judge models (default: {','.join(DEFAULT_JUDGES)})"
    )
    parser.add_argument(
        "--recovery-only",
        action="store_true",
        help="Only show recovery analysis"
    )

    args = parser.parse_args()

    judge_models = [j.strip() for j in args.judges.split(",")]

    print("=" * 80)
    print("EMERGENCY TEST ANALYSIS")
    print("=" * 80)
    print(f"Model: {args.model}")
    print(f"Pass: {args.pass_number}")
    print("=" * 80)

    # Score comparison
    if not args.recovery_only:
        if args.evaluate:
            # Run evaluation
            scores = run_commonllm_judge(args.model, args.pass_number, judge_models)
        else:
            # Load existing scores
            scores = load_evaluation_scores(args.model, args.pass_number, judge_models)

        if scores:
            print_score_comparison(scores)
        else:
            print("\n⚠ No evaluation scores found. Run with --evaluate to generate them.")

    # Recovery analysis
    analyze_recovery(args.model, args.pass_number)

    print("\n" + "=" * 80)
    print("ANALYSIS COMPLETE")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
