#!/usr/bin/env python3
"""
Budget Constraint Experiment Analysis

Analyzes budget constraint test results to understand the tradeoff between
tool call efficiency and answer quality.

Analysis Pipeline:
1. Extract metrics from each trajectory:
   - cost_used: Number of non-search tool calls
   - compliance: 1 if cost_used ≤ budget, 0 otherwise
   - quality: Final answer score (from CommonLLMJudge with 3 judges)

2. Aggregate to model × budget level:
   - Q_B = mean(quality)
   - C_B = mean(cost_used)
   - R_B = mean(compliance) - compliance rate

3. Calculate 4 key metrics:
   - Low-budget quality: Q3 (quality under tight budget)
   - Compliance: R3, R5, R7 (who follows constraints)
   - Marginal benefit (tradeoff core):
     * slope_3to5 = (Q5-Q3)/(C5-C3)
     * slope_5to7 = (Q7-Q5)/(C7-C5)
   - Overall tradeoff: AUC(Q vs C) - area under curve

4. Visualization:
   - Plot: Quality vs Cost curve for each model (3 points: budget_3/5/7)
   - Table: All metrics per model

Usage:
    # 1. Run evaluation only (generate scores)
    python analyze_scripts/analyze_budget_experiment.py --evaluate \
        --model google/gemini-3-pro-preview \
        --pass-number 1

    # 2. Analyze existing evaluation results
    python analyze_scripts/analyze_budget_experiment.py --analyze \
        --model google/gemini-3-pro-preview \
        --pass-number 1

    # 3. Full pipeline (evaluate + analyze)
    python analyze_scripts/analyze_budget_experiment.py --full \
        --model google/gemini-3-pro-preview \
        --pass-number 1

    # 4. Compare multiple models
    python analyze_scripts/analyze_budget_experiment.py --analyze \
        --models "google/gemini-pro-preview,openai/gpt-4o,deepseek/deepseek-v3.2" \
        --pass-number 1

Data Requirements:
    - Trajectories: trajectories/Budget_test/{model}/{budget_level}/pass@{N}/*.json
    - Generated by: python runtime/budget_constraint_test.py

Output:
    - Evaluation results: evaluation/Budget_test/{model}/{budget_level}/pass@{N}/{judge}/
    - Analysis plots: analysis/Budget_test/quality_vs_cost_{timestamp}.png
    - Summary table: analysis/Budget_test/summary_{timestamp}.json
"""
from __future__ import annotations

import sys
import os
import json
import argparse
import subprocess
import statistics
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from collections import defaultdict
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

BUDGET_LEVELS = ["budget_3", "budget_5", "budget_7", "baseline"]
BUDGET_CONSTRAINTS = {
    "budget_3": 3,
    "budget_5": 5,
    "budget_7": 7,
    "baseline": float('inf')
}

DEFAULT_JUDGES = [
    "openai/gpt-4o-mini",
    "deepseek/deepseek-v3.2",
    "openai/gpt-5.1-chat"
]

# Search tool patterns (these are FREE - don't count toward budget)
SEARCH_TOOL_KEYWORDS = [
    "search", "find", "query", "lookup", "discover",
    "list", "explore", "browse", "scan"
]


def is_search_tool(tool_name: str) -> bool:
    """Check if a tool is a search tool (free, doesn't consume budget)."""
    tool_lower = tool_name.lower()
    return any(keyword in tool_lower for keyword in SEARCH_TOOL_KEYWORDS)


def extract_trajectory_metrics(traj_path: Path, budget: int) -> Dict[str, Any]:
    """
    Extract cost_used, compliance, and metadata from a trajectory.

    Returns:
        {
            "cost_used": int,        # Number of non-search tool calls
            "compliance": int,       # 1 if cost_used <= budget, else 0
            "query_uuid": str,
            "has_final_answer": bool
        }
    """
    try:
        with open(traj_path, 'r') as f:
            traj = json.load(f)

        # Count non-search tool calls
        cost_used = 0
        steps = traj.get("steps", [])

        for step in steps:
            action = step.get("action", {})
            tool_name = action.get("tool", "")

            # Only count non-search tools
            if tool_name and not is_search_tool(tool_name):
                cost_used += 1

        # Check compliance
        compliance = 1 if cost_used <= budget else 0

        # Extract metadata
        metadata = traj.get("metadata", {})
        query_uuid = metadata.get("query_uuid", "")

        # Check if final answer exists
        final_answer = traj.get("final_answer", "")
        has_final_answer = bool(final_answer and final_answer.strip())

        return {
            "cost_used": cost_used,
            "compliance": compliance,
            "query_uuid": query_uuid,
            "has_final_answer": has_final_answer
        }

    except Exception as e:
        print(f"  ⚠ Error processing {traj_path.name}: {e}")
        return None


def run_commonllm_judge(
    model: str,
    pass_number: int,
    budget_levels: List[str],
    judge_models: List[str],
) -> Dict[str, Dict[str, List[float]]]:
    """
    Run CommonLLMJudge for specified budget levels with multiple judges.

    Returns:
        Dict[budget_level, Dict[query_uuid, List[scores_from_judges]]]
    """
    print("\n" + "=" * 80)
    print("RUNNING COMMONLLMJUDGE EVALUATION")
    print("=" * 80)
    print(f"Model: {model}")
    print(f"Pass number: {pass_number}")
    print(f"Judges: {', '.join(judge_models)}\n")

    budget_dir = PROJECT_ROOT / "trajectories" / "Budget_test" / model.replace("/", "-")
    results = {}

    for budget_level in budget_levels:
        print(f"\n{'='*70}")
        print(f"Budget Level: {budget_level}")
        print(f"{'='*70}")

        traj_dir = budget_dir / budget_level / f"pass@{pass_number}"
        if not traj_dir.exists():
            print(f"  ⚠ Directory not found: {traj_dir}")
            continue

        budget_scores = defaultdict(list)  # query_uuid -> [score1, score2, score3]

        for judge_model in judge_models:
            print(f"\n  Judge: {judge_model}")

            # Output directory for evaluations
            eval_output_dir = (
                PROJECT_ROOT / "evaluation" / "Budget_test" / model.replace("/", "-") /
                budget_level / f"pass@{pass_number}" / judge_model.replace("/", "-")
            )
            eval_output_dir.mkdir(parents=True, exist_ok=True)

            # Run CommonLLMJudge
            cmd = [
                "python",
                str(PROJECT_ROOT / "Orchestrator/mcpuniverse/evaluator/commonllmjudge.py"),
                "--traj_dir", str(traj_dir),
                "--model", judge_model,
                "--step-by-step",
                "--recursive",
                "--output-dir", str(eval_output_dir),
                "--parallel", "10",
            ]

            try:
                print(f"    Running evaluation...")
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=600,
                    cwd=str(PROJECT_ROOT)
                )

                if result.returncode != 0:
                    print(f"    ✗ Error: {result.stderr[:200]}")
                    continue

                print(f"    ✓ Completed")

            except subprocess.TimeoutExpired:
                print(f"    ✗ Timeout")
                continue
            except Exception as e:
                print(f"    ✗ Error: {e}")
                continue

            # Read evaluation results
            eval_files = list(eval_output_dir.glob("**/eval_*.json"))

            for eval_file in eval_files:
                try:
                    with open(eval_file, 'r') as f:
                        eval_data = json.load(f)

                    # Extract query UUID from filename: eval_<uuid>.json
                    filename = eval_file.stem
                    if filename.startswith("eval_"):
                        query_uuid = filename[5:]  # Remove "eval_" prefix
                        # Get score from final_answer_evaluation
                        final_answer_eval = eval_data.get("final_answer_evaluation", {})
                        score = final_answer_eval.get("final_answer_score", 0.0)
                        budget_scores[query_uuid].append(score)

                except Exception as e:
                    print(f"    ⚠ Error reading {eval_file.name}: {e}")

        results[budget_level] = dict(budget_scores)
        print(f"\n  Total evaluated: {len(budget_scores)} queries")

    return results


def aggregate_metrics(
    model: str,
    pass_number: int,
    budget_levels: List[str],
    evaluation_results: Optional[Dict[str, Dict[str, List[float]]]] = None
) -> Dict[str, Dict[str, float]]:
    """
    Aggregate metrics for each model × budget level.

    Returns:
        Dict[budget_level, {
            "Q": mean_quality,
            "C": mean_cost,
            "R": compliance_rate,
            "n": sample_size
        }]
    """
    print("\n" + "=" * 80)
    print("AGGREGATING METRICS")
    print("=" * 80)

    budget_dir = PROJECT_ROOT / "trajectories" / "Budget_test" / model.replace("/", "-")
    aggregated = {}

    for budget_level in budget_levels:
        print(f"\n{budget_level}:")

        budget = BUDGET_CONSTRAINTS[budget_level]
        traj_dir = budget_dir / budget_level / f"pass@{pass_number}"

        if not traj_dir.exists():
            print(f"  ⚠ Directory not found: {traj_dir}")
            continue

        # Collect trajectory metrics
        traj_files = list(traj_dir.glob("trajectory_*.json"))

        costs = []
        compliances = []
        qualities = []

        for traj_file in traj_files:
            metrics = extract_trajectory_metrics(traj_file, budget)
            if not metrics:
                continue

            query_uuid = metrics["query_uuid"]
            cost_used = metrics["cost_used"]
            compliance = metrics["compliance"]

            costs.append(cost_used)
            compliances.append(compliance)

            # Get quality score if available
            if evaluation_results and budget_level in evaluation_results:
                if query_uuid in evaluation_results[budget_level]:
                    scores = evaluation_results[budget_level][query_uuid]
                    # Average across judges
                    avg_score = sum(scores) / len(scores) if scores else 0.0
                    qualities.append(avg_score)

        if not costs:
            print(f"  ⚠ No valid trajectories found")
            continue

        # Calculate aggregated metrics
        Q = statistics.mean(qualities) if qualities else None
        C = statistics.mean(costs)
        R = statistics.mean(compliances)
        n = len(costs)

        aggregated[budget_level] = {
            "Q": Q,
            "C": C,
            "R": R,
            "n": n
        }

        print(f"  Sample size: {n}")
        print(f"  Mean cost (C): {C:.2f}")
        print(f"  Compliance rate (R): {R:.2%}")
        if Q is not None:
            print(f"  Mean quality (Q): {Q:.3f}")
        else:
            print(f"  Mean quality (Q): Not evaluated yet")

    return aggregated


def calculate_key_metrics(
    aggregated: Dict[str, Dict[str, float]]
) -> Dict[str, Any]:
    """
    Calculate 4 key metrics from aggregated data.

    Returns:
        {
            "Q3": float,              # Low-budget quality
            "R3": float,              # Compliance at budget=3
            "R5": float,              # Compliance at budget=5
            "R7": float,              # Compliance at budget=7
            "slope_3to5": float,      # Marginal benefit
            "slope_5to7": float,      # Marginal benefit
            "AUC": float              # Area under Q vs C curve
        }
    """
    metrics = {}

    # Extract values for budget 3, 5, 7 (skip baseline)
    budget_3 = aggregated.get("budget_3", {})
    budget_5 = aggregated.get("budget_5", {})
    budget_7 = aggregated.get("budget_7", {})

    # 1. Low-budget quality
    metrics["Q3"] = budget_3.get("Q")
    metrics["Q5"] = budget_5.get("Q")
    metrics["Q7"] = budget_7.get("Q")

    # 2. Compliance rates
    metrics["R3"] = budget_3.get("R")
    metrics["R5"] = budget_5.get("R")
    metrics["R7"] = budget_7.get("R")

    # 3. Marginal benefits (slopes)
    C3, Q3 = budget_3.get("C"), budget_3.get("Q")
    C5, Q5 = budget_5.get("C"), budget_5.get("Q")
    C7, Q7 = budget_7.get("C"), budget_7.get("Q")

    if all(v is not None for v in [C3, C5, Q3, Q5]) and C5 != C3:
        metrics["slope_3to5"] = (Q5 - Q3) / (C5 - C3)
    else:
        metrics["slope_3to5"] = None

    if all(v is not None for v in [C5, C7, Q5, Q7]) and C7 != C5:
        metrics["slope_5to7"] = (Q7 - Q5) / (C7 - C5)
    else:
        metrics["slope_5to7"] = None

    # 4. AUC (area under Q vs C curve using trapezoidal rule)
    if all(v is not None for v in [C3, C5, C7, Q3, Q5, Q7]):
        # Sort by cost
        points = sorted([(C3, Q3), (C5, Q5), (C7, Q7)], key=lambda p: p[0])

        # Trapezoidal rule
        auc = 0.0
        for i in range(len(points) - 1):
            x1, y1 = points[i]
            x2, y2 = points[i + 1]
            auc += (x2 - x1) * (y1 + y2) / 2

        metrics["AUC"] = auc
    else:
        metrics["AUC"] = None

    return metrics


def plot_quality_vs_cost(
    results: Dict[str, Dict[str, Dict[str, float]]],
    output_path: Path
):
    """
    Plot Quality vs Cost curves for all models.

    Args:
        results: Dict[model_name, Dict[budget_level, aggregated_metrics]]
        output_path: Path to save the plot
    """
    plt.figure(figsize=(10, 6))

    for model_name, aggregated in results.items():
        # Extract points for budget 3, 5, 7
        costs = []
        qualities = []

        for budget_level in ["budget_3", "budget_5", "budget_7"]:
            if budget_level in aggregated:
                C = aggregated[budget_level].get("C")
                Q = aggregated[budget_level].get("Q")
                if C is not None and Q is not None:
                    costs.append(C)
                    qualities.append(Q)

        if len(costs) >= 2:
            # Plot line with markers
            label = model_name.split("/")[-1] if "/" in model_name else model_name
            plt.plot(costs, qualities, marker='o', linewidth=2, markersize=8, label=label)

    plt.xlabel("Mean Cost (Non-search Tool Calls)", fontsize=12)
    plt.ylabel("Mean Quality (Judge Score)", fontsize=12)
    plt.title("Quality vs Cost Tradeoff", fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # Save plot
    output_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\n✓ Plot saved to: {output_path}")

    plt.close()


def print_summary_table(
    results: Dict[str, Dict[str, Any]]
):
    """
    Print a summary table with all metrics.

    Args:
        results: Dict[model_name, key_metrics]
    """
    print("\n" + "=" * 120)
    print("SUMMARY TABLE")
    print("=" * 120)

    # Header
    header = f"{'Model':<30} {'Q3':>8} {'Q5':>8} {'Q7':>8} {'R3':>8} {'R5':>8} {'R7':>8} {'Slope3→5':>10} {'Slope5→7':>10} {'AUC':>10}"
    print(header)
    print("-" * 120)

    # Rows
    for model_name, metrics in results.items():
        model_short = model_name.split("/")[-1] if "/" in model_name else model_name

        Q3 = f"{metrics['Q3']:.3f}" if metrics['Q3'] is not None else "N/A"
        Q5 = f"{metrics['Q5']:.3f}" if metrics['Q5'] is not None else "N/A"
        Q7 = f"{metrics['Q7']:.3f}" if metrics['Q7'] is not None else "N/A"

        R3 = f"{metrics['R3']:.2%}" if metrics['R3'] is not None else "N/A"
        R5 = f"{metrics['R5']:.2%}" if metrics['R5'] is not None else "N/A"
        R7 = f"{metrics['R7']:.2%}" if metrics['R7'] is not None else "N/A"

        slope_3to5 = f"{metrics['slope_3to5']:.4f}" if metrics['slope_3to5'] is not None else "N/A"
        slope_5to7 = f"{metrics['slope_5to7']:.4f}" if metrics['slope_5to7'] is not None else "N/A"

        auc = f"{metrics['AUC']:.3f}" if metrics['AUC'] is not None else "N/A"

        row = f"{model_short:<30} {Q3:>8} {Q5:>8} {Q7:>8} {R3:>8} {R5:>8} {R7:>8} {slope_3to5:>10} {slope_5to7:>10} {auc:>10}"
        print(row)

    print("=" * 120)

    # Interpretation guide
    print("\nMetrics Interpretation:")
    print("  Q3/Q5/Q7:      Mean quality at each budget level (higher is better)")
    print("  R3/R5/R7:      Compliance rate - % of queries within budget (higher is better)")
    print("  Slope3→5/5→7:  Marginal quality gain per extra tool call (higher is better)")
    print("  AUC:           Overall quality-cost tradeoff efficiency (higher is better)")
    print("\nKey Questions:")
    print("  - Who's cheap & good?     → High AUC, high Q3")
    print("  - Who's budget-conscious? → High R3/R5/R7")
    print("  - Who relies on tools?    → High slopes (gains a lot from more calls)")


def analyze_budget_experiment(
    models: List[str],
    pass_number: int,
    budget_levels: List[str],
    evaluation_results: Optional[Dict[str, Dict[str, Dict[str, List[float]]]]] = None
):
    """
    Main analysis function.

    Args:
        models: List of model names
        pass_number: Pass number
        budget_levels: List of budget levels to analyze
        evaluation_results: Optional pre-computed evaluation results
                           Dict[model, Dict[budget_level, Dict[query_uuid, scores]]]
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    all_aggregated = {}
    all_metrics = {}

    # Process each model
    for model in models:
        print(f"\n{'#'*80}")
        print(f"MODEL: {model}")
        print(f"{'#'*80}")

        # Get evaluation results for this model
        model_eval = evaluation_results.get(model) if evaluation_results else None

        # Aggregate metrics
        aggregated = aggregate_metrics(model, pass_number, budget_levels, model_eval)

        # Calculate key metrics
        metrics = calculate_key_metrics(aggregated)

        all_aggregated[model] = aggregated
        all_metrics[model] = metrics

    # Generate visualization
    analysis_dir = PROJECT_ROOT / "analysis" / "Budget_test"
    plot_path = analysis_dir / f"quality_vs_cost_{timestamp}.png"
    plot_quality_vs_cost(all_aggregated, plot_path)

    # Print summary table
    print_summary_table(all_metrics)

    # Save summary JSON
    summary_path = analysis_dir / f"summary_{timestamp}.json"
    summary_data = {
        "timestamp": datetime.now().isoformat(),
        "models": models,
        "pass_number": pass_number,
        "budget_levels": budget_levels,
        "aggregated": all_aggregated,
        "metrics": all_metrics
    }

    summary_path.parent.mkdir(parents=True, exist_ok=True)
    with open(summary_path, 'w') as f:
        json.dump(summary_data, f, indent=2)

    print(f"\n✓ Summary saved to: {summary_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze budget constraint experiment results"
    )

    # Mode selection
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument("--evaluate", action="store_true",
                           help="Run CommonLLMJudge evaluation only")
    mode_group.add_argument("--analyze", action="store_true",
                           help="Analyze existing evaluation results")
    mode_group.add_argument("--full", action="store_true",
                           help="Full pipeline: evaluate + analyze")

    # Model configuration
    parser.add_argument("--model", type=str,
                       help="Single model to analyze")
    parser.add_argument("--models", type=str,
                       help="Comma-separated list of models to compare")

    # Experiment configuration
    parser.add_argument("--pass-number", type=int, default=1,
                       help="Pass number (default: 1)")
    parser.add_argument("--budget-levels", type=str,
                       default="budget_3,budget_5,budget_7",
                       help="Comma-separated budget levels (default: budget_3,budget_5,budget_7)")

    # Judge configuration
    parser.add_argument("--judges", type=str,
                       default=",".join(DEFAULT_JUDGES),
                       help=f"Comma-separated judge models (default: {','.join(DEFAULT_JUDGES)})")

    args = parser.parse_args()

    # Parse models
    if args.models:
        models = [m.strip() for m in args.models.split(",")]
    elif args.model:
        models = [args.model]
    else:
        parser.error("Either --model or --models must be specified")

    # Parse budget levels
    budget_levels = [b.strip() for b in args.budget_levels.split(",")]

    # Parse judges
    judge_models = [j.strip() for j in args.judges.split(",")]

    print("=" * 80)
    print("BUDGET CONSTRAINT EXPERIMENT ANALYSIS")
    print("=" * 80)
    print(f"Models:        {', '.join(models)}")
    print(f"Pass number:   {args.pass_number}")
    print(f"Budget levels: {', '.join(budget_levels)}")
    if args.evaluate or args.full:
        print(f"Judges:        {', '.join(judge_models)}")
    print("=" * 80)

    evaluation_results = {}

    # Step 1: Evaluation
    if args.evaluate or args.full:
        for model in models:
            print(f"\n{'#'*80}")
            print(f"EVALUATING MODEL: {model}")
            print(f"{'#'*80}")

            results = run_commonllm_judge(
                model=model,
                pass_number=args.pass_number,
                budget_levels=budget_levels,
                judge_models=judge_models
            )
            evaluation_results[model] = results

    # Step 2: Analysis
    if args.analyze or args.full:
        analyze_budget_experiment(
            models=models,
            pass_number=args.pass_number,
            budget_levels=budget_levels,
            evaluation_results=evaluation_results if evaluation_results else None
        )


if __name__ == "__main__":
    main()
