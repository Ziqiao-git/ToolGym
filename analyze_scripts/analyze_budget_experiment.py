#!/usr/bin/env python3
"""
Budget Constraint Experiment Analysis

Analyzes budget constraint test results to understand the tradeoff between
tool call efficiency and answer quality.

Analysis Pipeline:
1. Extract metrics from each trajectory:
   - cost_used: Number of non-search tool calls
   - compliance: 1 if cost_used â‰¤ budget, 0 otherwise
   - quality: Final answer score (from CommonLLMJudge with 3 judges)

2. Aggregate to model Ã— budget level:
   - Q_B = mean(quality)
   - C_B = mean(cost_used)
   - R_B = mean(compliance) - compliance rate

3. Calculate 4 key metrics:
   - Low-budget quality: Q3 (quality under tight budget)
   - Compliance: R3, R5, R7 (who follows constraints)
   - Marginal benefit (tradeoff core):
     * slope_3to5 = (Q5-Q3)/(C5-C3)
     * slope_5to7 = (Q7-Q5)/(C7-C5)
   - Overall tradeoff: AUC(Q vs C) - area under curve

4. Visualization:
   - Plot: Quality vs Cost curve for each model (3 points: budget_3/5/7)
   - Table: All metrics per model

Usage:
    # 1. Run evaluation only (generate scores)
    python analyze_scripts/analyze_budget_experiment.py --evaluate \
        --model google/gemini-3-pro-preview \
        --pass-number 1

    # 2. Analyze existing evaluation results
    python analyze_scripts/analyze_budget_experiment.py --analyze \
        --model google/gemini-3-pro-preview \
        --pass-number 1

    # 3. Full pipeline (evaluate + analyze)
    python analyze_scripts/analyze_budget_experiment.py --full \
        --model google/gemini-3-pro-preview \
        --pass-number 1

    # 4. Compare multiple models
    python analyze_scripts/analyze_budget_experiment.py --analyze \
        --models "google/gemini-pro-preview,openai/gpt-4o,deepseek/deepseek-v3.2" \
        --pass-number 1

Data Requirements:
    - Trajectories: trajectories/Budget_test/{model}/{budget_level}/pass@{N}/*.json
    - Generated by: python runtime/budget_constraint_test.py

Output:
    - Evaluation results: evaluation/Budget_test/{model}/{budget_level}/pass@{N}/{judge}/
    - Analysis plots: analysis/Budget_test/quality_vs_cost_{timestamp}.png
    - Summary table: analysis/Budget_test/summary_{timestamp}.json
"""
from __future__ import annotations

import sys
import os
import json
import argparse
import subprocess
import statistics
from pathlib import Path
from collections import defaultdict
from typing import Dict, Any, List, Optional

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

BUDGET_LEVELS = ["budget_3", "budget_5", "budget_7", "baseline"]
BUDGET_CONSTRAINTS = {
    "budget_3": 3,
    "budget_5": 5,
    "budget_7": 7,
    "baseline": float('inf')
}

DEFAULT_JUDGES = [
    "openai/gpt-4o-mini",
    "deepseek/deepseek-v3.2",
    "openai/gpt-5.1-chat"
]

# Search tool patterns (these are FREE - don't count toward budget)
SEARCH_TOOL_KEYWORDS = [
    "search", "find", "query", "lookup", "discover",
    "list", "explore", "browse", "scan"
]


def is_search_tool(tool_name: str) -> bool:
    """Check if a tool is a search tool (free, doesn't consume budget)."""
    tool_lower = tool_name.lower()
    return any(keyword in tool_lower for keyword in SEARCH_TOOL_KEYWORDS)


def extract_trajectory_metrics(traj_path: Path, budget: int) -> Dict[str, Any]:
    """
    Extract cost_used, compliance, and metadata from a trajectory.

    Returns:
        {
            "cost_used": int,        # Number of non-search tool calls
            "compliance": int,       # 1 if cost_used <= budget, else 0
            "query_uuid": str,
            "has_final_answer": bool
        }
    """
    try:
        with open(traj_path, 'r') as f:
            traj = json.load(f)

        # Count non-search tool calls
        cost_used = 0

        # Try new format first (execution.tool_calls)
        if "execution" in traj:
            tool_calls = traj.get("execution", {}).get("tool_calls", [])
            for call in tool_calls:
                tool_name = call.get("tool", "")
                # Only count non-search tools
                if tool_name and not is_search_tool(tool_name):
                    cost_used += 1
        else:
            # Fallback to old format (steps)
            steps = traj.get("steps", [])
            for step in steps:
                action = step.get("action", {})
                tool_name = action.get("tool", "")
                # Only count non-search tools
                if tool_name and not is_search_tool(tool_name):
                    cost_used += 1

        # Check compliance
        compliance = 1 if cost_used <= budget else 0

        # Extract metadata
        metadata = traj.get("metadata", {})
        query_uuid = metadata.get("query_uuid", "")

        # Check if final answer exists
        # Try new format first
        final_answer = traj.get("execution", {}).get("final_response", "")
        if not final_answer:
            # Fallback to old format
            final_answer = traj.get("final_answer", "")
        has_final_answer = bool(final_answer and final_answer.strip())

        return {
            "cost_used": cost_used,
            "compliance": compliance,
            "query_uuid": query_uuid,
            "has_final_answer": has_final_answer
        }

    except Exception as e:
        print(f"  âš  Error processing {traj_path.name}: {e}")
        return None


def run_commonllm_judge(
    model: str,
    pass_number: int,
    budget_levels: List[str],
    judge_models: List[str],
) -> Dict[str, Dict[str, List[float]]]:
    """
    Run CommonLLMJudge for specified budget levels with multiple judges.

    Returns:
        Dict[budget_level, Dict[query_uuid, List[scores_from_judges]]]
    """
    print("\n" + "=" * 80)
    print("RUNNING COMMONLLMJUDGE EVALUATION")
    print("=" * 80)
    print(f"Model: {model}")
    print(f"Pass number: {pass_number}")
    print(f"Judges: {', '.join(judge_models)}\n")

    budget_dir = PROJECT_ROOT / "trajectories" / "Budget_test" / model.replace("/", "-")
    results = {}

    for budget_level in budget_levels:
        print(f"\n{'='*70}")
        print(f"Budget Level: {budget_level}")
        print(f"{'='*70}")

        traj_dir = budget_dir / budget_level / f"pass@{pass_number}"
        if not traj_dir.exists():
            print(f"  âš  Directory not found: {traj_dir}")
            continue

        budget_scores = defaultdict(list)  # query_uuid -> [score1, score2, score3]

        for judge_model in judge_models:
            print(f"\n  Judge: {judge_model}")

            # Output directory for evaluations
            eval_output_dir = (
                PROJECT_ROOT / "evaluation" / "Budget_test" / model.replace("/", "-") /
                budget_level / f"pass@{pass_number}" / judge_model.replace("/", "-")
            )
            eval_output_dir.mkdir(parents=True, exist_ok=True)

            # Run CommonLLMJudge
            cmd = [
                "python",
                str(PROJECT_ROOT / "Orchestrator/mcpuniverse/evaluator/commonllmjudge.py"),
                "--traj_dir", str(traj_dir),
                "--model", judge_model,
                "--step-by-step",
                "--recursive",
                "--output-dir", str(eval_output_dir),
                "--parallel", "10",
            ]

            try:
                print(f"    Running evaluation...")
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=600,
                    cwd=str(PROJECT_ROOT)
                )

                if result.returncode != 0:
                    print(f"    âœ— Error: {result.stderr[:200]}")
                    continue

                print(f"    âœ“ Completed")

            except subprocess.TimeoutExpired:
                print(f"    âœ— Timeout")
                continue
            except Exception as e:
                print(f"    âœ— Error: {e}")
                continue

            # Read evaluation results
            eval_files = list(eval_output_dir.glob("**/eval_*.json"))

            for eval_file in eval_files:
                try:
                    with open(eval_file, 'r') as f:
                        eval_data = json.load(f)

                    # Extract query UUID from filename: eval_<uuid>.json
                    filename = eval_file.stem
                    if filename.startswith("eval_"):
                        query_uuid = filename[5:]  # Remove "eval_" prefix
                        # Get score from final_answer_evaluation
                        final_answer_eval = eval_data.get("final_answer_evaluation", {})
                        score = final_answer_eval.get("final_answer_score", 0.0)
                        budget_scores[query_uuid].append(score)

                except Exception as e:
                    print(f"    âš  Error reading {eval_file.name}: {e}")

        results[budget_level] = dict(budget_scores)
        print(f"\n  Total evaluated: {len(budget_scores)} queries")

    return results


def load_evaluation_scores(
    model: str,
    pass_number: int,
    budget_levels: List[str],
    judge_models: List[str]
) -> Dict[str, Dict[str, List[float]]]:
    """
    Load existing evaluation scores from disk.

    Returns:
        Dict[budget_level, Dict[query_uuid, List[scores_from_judges]]]
    """
    print("\n" + "=" * 80)
    print("LOADING EXISTING EVALUATION SCORES")
    print("=" * 80 + "\n")

    results = {}

    for budget_level in budget_levels:
        budget_scores = defaultdict(list)

        for judge_model in judge_models:
            eval_output_dir = (
                PROJECT_ROOT / "evaluation" / "Budget_test" / model.replace("/", "-") /
                budget_level / f"pass@{pass_number}" / judge_model.replace("/", "-")
            )

            if not eval_output_dir.exists():
                print(f"âš  Not found: {budget_level}/{judge_model.split('/')[-1]}")
                continue

            # Read from nested directory structure
            eval_files = list(eval_output_dir.glob("**/eval_*.json"))

            for eval_file in eval_files:
                try:
                    with open(eval_file, 'r') as f:
                        eval_data = json.load(f)

                    filename = eval_file.stem
                    if filename.startswith("eval_"):
                        query_uuid = filename[5:]  # Remove "eval_" prefix
                        # Get score from final_answer_evaluation
                        final_answer_eval = eval_data.get("final_answer_evaluation", {})
                        score = final_answer_eval.get("final_answer_score", 0.0)
                        budget_scores[query_uuid].append(score)

                except Exception as e:
                    continue

            print(f"âœ“ Loaded {len(eval_files)} scores: {budget_level}/{judge_model.split('/')[-1]}")

        results[budget_level] = dict(budget_scores)

    return results


def aggregate_metrics(
    model: str,
    pass_number: int,
    budget_levels: List[str],
    evaluation_results: Optional[Dict[str, Dict[str, List[float]]]] = None
) -> Dict[str, Dict[str, float]]:
    """
    Aggregate metrics for each model Ã— budget level.

    Returns:
        Dict[budget_level, {
            "Q": mean_quality,
            "C": mean_cost,
            "R": compliance_rate,
            "n": sample_size
        }]
    """
    print("\n" + "=" * 80)
    print("AGGREGATING METRICS")
    print("=" * 80)

    budget_dir = PROJECT_ROOT / "trajectories" / "Budget_test" / model.replace("/", "-")
    aggregated = {}

    for budget_level in budget_levels:
        print(f"\n{budget_level}:")

        budget = BUDGET_CONSTRAINTS[budget_level]
        traj_dir = budget_dir / budget_level / f"pass@{pass_number}"

        if not traj_dir.exists():
            print(f"  âš  Directory not found: {traj_dir}")
            continue

        # Collect trajectory metrics
        traj_files = list(traj_dir.glob("trajectory_*.json"))

        costs = []
        compliances = []
        qualities = []

        for traj_file in traj_files:
            metrics = extract_trajectory_metrics(traj_file, budget)
            if not metrics:
                continue

            query_uuid = metrics["query_uuid"]
            cost_used = metrics["cost_used"]
            compliance = metrics["compliance"]

            costs.append(cost_used)
            compliances.append(compliance)

            # Get quality score if available
            if evaluation_results and budget_level in evaluation_results:
                if query_uuid in evaluation_results[budget_level]:
                    scores = evaluation_results[budget_level][query_uuid]
                    # Average across judges
                    avg_score = sum(scores) / len(scores) if scores else 0.0
                    qualities.append(avg_score)

        if not costs:
            print(f"  âš  No valid trajectories found")
            continue

        # Calculate aggregated metrics
        Q = statistics.mean(qualities) if qualities else None
        C = statistics.mean(costs)
        R = statistics.mean(compliances)
        n = len(costs)

        aggregated[budget_level] = {
            "Q": Q,
            "C": C,
            "R": R,
            "n": n
        }

        print(f"  Sample size: {n}")
        print(f"  Mean cost (C): {C:.2f}")
        print(f"  Compliance rate (R): {R:.2%}")
        if Q is not None:
            print(f"  Mean quality (Q): {Q:.3f}")
        else:
            print(f"  Mean quality (Q): Not evaluated yet")

    return aggregated


def calculate_key_metrics(
    aggregated: Dict[str, Dict[str, float]]
) -> Dict[str, Any]:
    """
    Calculate 4 key metrics from aggregated data.

    Returns:
        {
            "Q3": float,              # Low-budget quality
            "R3": float,              # Compliance at budget=3
            "R5": float,              # Compliance at budget=5
            "R7": float,              # Compliance at budget=7
            "slope_3to5": float,      # Marginal benefit
            "slope_5to7": float,      # Marginal benefit
            "AUC": float              # Area under Q vs C curve
        }
    """
    metrics = {}

    # Extract values for budget 3, 5, 7 (skip baseline)
    budget_3 = aggregated.get("budget_3", {})
    budget_5 = aggregated.get("budget_5", {})
    budget_7 = aggregated.get("budget_7", {})

    # 1. Low-budget quality
    metrics["Q3"] = budget_3.get("Q")
    metrics["Q5"] = budget_5.get("Q")
    metrics["Q7"] = budget_7.get("Q")

    # 2. Compliance rates
    metrics["R3"] = budget_3.get("R")
    metrics["R5"] = budget_5.get("R")
    metrics["R7"] = budget_7.get("R")

    # 3. Marginal benefits (slopes)
    C3, Q3 = budget_3.get("C"), budget_3.get("Q")
    C5, Q5 = budget_5.get("C"), budget_5.get("Q")
    C7, Q7 = budget_7.get("C"), budget_7.get("Q")

    if all(v is not None for v in [C3, C5, Q3, Q5]) and C5 != C3:
        metrics["slope_3to5"] = (Q5 - Q3) / (C5 - C3)
    else:
        metrics["slope_3to5"] = None

    if all(v is not None for v in [C5, C7, Q5, Q7]) and C7 != C5:
        metrics["slope_5to7"] = (Q7 - Q5) / (C7 - C5)
    else:
        metrics["slope_5to7"] = None

    # 4. AUC (area under Q vs C curve using trapezoidal rule)
    if all(v is not None for v in [C3, C5, C7, Q3, Q5, Q7]):
        # Sort by cost
        points = sorted([(C3, Q3), (C5, Q5), (C7, Q7)], key=lambda p: p[0])

        # Trapezoidal rule
        auc = 0.0
        for i in range(len(points) - 1):
            x1, y1 = points[i]
            x2, y2 = points[i + 1]
            auc += (x2 - x1) * (y1 + y2) / 2

        metrics["AUC"] = auc
    else:
        metrics["AUC"] = None

    return metrics


def print_summary_table(
    results: Dict[str, Dict[str, Any]],
    aggregated: Dict[str, Dict[str, Dict[str, float]]]
):
    """
    Print a detailed summary table with all metrics.

    Args:
        results: Dict[model_name, key_metrics]
        aggregated: Dict[model_name, Dict[budget_level, aggregated_metrics]]
    """
    print("\n" + "=" * 140)
    print(" " * 50 + "BUDGET CONSTRAINT EXPERIMENT RESULTS")
    print("=" * 140)

    for model_name in results.keys():
        model_short = model_name.split("/")[-1] if "/" in model_name else model_name
        metrics = results[model_name]
        model_agg = aggregated[model_name]

        print(f"\nâ”Œâ”€ Model: {model_short}")
        print("â”‚")

        # Quality & Cost per Budget Level
        print("â”‚  â”Œâ”€ Quality & Cost per Budget Level:")
        print("â”‚  â”‚")
        print("â”‚  â”‚  Budget Level  â”‚  Mean Quality (Q)  â”‚  Mean Cost (C)  â”‚  Compliance Rate (R)  â”‚  Sample Size")
        print("â”‚  â”‚  " + "â”€" * 95)

        for budget_level in ["budget_3", "budget_5", "budget_7", "baseline"]:
            if budget_level in model_agg:
                agg = model_agg[budget_level]
                Q = agg.get("Q")
                C = agg.get("C")
                R = agg.get("R")
                n = agg.get("n", 0)

                Q_str = f"{Q:.3f}" if Q is not None else "N/A    "
                C_str = f"{C:.2f}" if C is not None else "N/A   "
                R_str = f"{R:.1%}" if R is not None else "N/A     "

                if budget_level == "baseline":
                    print(f"â”‚  â”‚  Baseline    â”‚      {Q_str}         â”‚      {C_str}      â”‚        {R_str}         â”‚      {n}")
                else:
                    budget_num = budget_level.split("_")[1]
                    print(f"â”‚  â”‚  Budget {budget_num}    â”‚      {Q_str}         â”‚      {C_str}      â”‚        {R_str}         â”‚      {n}")

        print("â”‚  â”‚")

        # Marginal Benefits (Slopes)
        print("â”‚  â”œâ”€ Marginal Benefits (Quality gain per additional tool call):")
        print("â”‚  â”‚")

        slope_3to5 = metrics['slope_3to5']
        slope_5to7 = metrics['slope_5to7']

        slope_3to5_str = f"{slope_3to5:.4f}" if slope_3to5 is not None else "N/A"
        slope_5to7_str = f"{slope_5to7:.4f}" if slope_5to7 is not None else "N/A"

        print(f"â”‚  â”‚    Budget 3 â†’ 5:  {slope_3to5_str} quality gain per tool call")
        print(f"â”‚  â”‚    Budget 5 â†’ 7:  {slope_5to7_str} quality gain per tool call")
        print("â”‚  â”‚")

        # Overall Tradeoff
        print("â”‚  â””â”€ Overall Tradeoff:")
        print("â”‚")

        auc = metrics['AUC']
        auc_str = f"{auc:.3f}" if auc is not None else "N/A"

        print(f"â”‚       AUC (Area Under Curve):  {auc_str}")
        print("â”‚       â†’ Higher AUC = Better quality-cost tradeoff efficiency")
        print("â”‚")

        # Key Insights
        print("â”‚  ðŸ“Š Key Insights:")

        # Baseline comparison
        if "baseline" in model_agg:
            baseline_Q = model_agg["baseline"].get("Q")
            baseline_C = model_agg["baseline"].get("C")
            Q3 = metrics.get('Q3')

            if baseline_Q is not None and Q3 is not None:
                quality_retention = (Q3 / baseline_Q) * 100 if baseline_Q > 0 else 0
                print(f"â”‚     â€¢ Baseline Quality: {baseline_Q:.3f} (Cost: {baseline_C:.2f})")
                print(f"â”‚     â€¢ Budget 3 retains {quality_retention:.1f}% of baseline quality")

                if quality_retention >= 90:
                    print("â”‚       âœ“ Excellent quality retention under tight budget")
                elif quality_retention >= 75:
                    print("â”‚       ~ Good quality retention under tight budget")
                else:
                    print("â”‚       âœ— Significant quality drop under tight budget")

        print("â”‚")

        # Budget consciousness
        R3 = metrics.get('R3', 0)
        R5 = metrics.get('R5', 0)
        R7 = metrics.get('R7', 0)
        if all(r is not None and r >= 0.95 for r in [R3, R5, R7]):
            print("â”‚     âœ“ Excellent budget compliance (>95% across all levels)")
        elif all(r is not None and r >= 0.80 for r in [R3, R5, R7]):
            print("â”‚     ~ Good budget compliance (>80% across all levels)")
        else:
            print("â”‚     âœ— Budget compliance needs improvement")

        # Quality efficiency
        Q3 = metrics.get('Q3')
        if Q3 is not None and Q3 >= 0.7:
            print("â”‚     âœ“ High quality even with tight budget (Q3 â‰¥ 0.7)")
        elif Q3 is not None and Q3 >= 0.5:
            print("â”‚     ~ Moderate quality with tight budget (Q3 â‰¥ 0.5)")
        else:
            print("â”‚     âœ— Low quality with tight budget (Q3 < 0.5)")

        # Tool dependency
        if slope_5to7 is not None and slope_3to5 is not None:
            if slope_5to7 > slope_3to5 * 2:
                print("â”‚     âš  High tool dependency: Quality drops sharply with fewer tool calls")
            elif slope_5to7 < slope_3to5 * 0.5:
                print("â”‚     âœ“ Low tool dependency: Maintains quality with budget constraints")

        print("â””" + "â”€" * 139)

    print("\n" + "=" * 140)
    print("\nðŸ“– Metrics Guide:")
    print("   â€¢ Quality (Q):      Judge score from 0-1 (higher is better)")
    print("   â€¢ Cost (C):         Mean number of non-search tool calls")
    print("   â€¢ Compliance (R):   % of queries staying within budget")
    print("   â€¢ Slope:            Î”Q/Î”C = quality gain per additional tool call")
    print("   â€¢ AUC:              Area under quality-cost curve (efficiency measure)")
    print("=" * 140)


def analyze_budget_experiment(
    models: List[str],
    pass_number: int,
    budget_levels: List[str],
    evaluation_results: Optional[Dict[str, Dict[str, Dict[str, List[float]]]]] = None
):
    """
    Main analysis function.

    Args:
        models: List of model names
        pass_number: Pass number
        budget_levels: List of budget levels to analyze
        evaluation_results: Optional pre-computed evaluation results
                           Dict[model, Dict[budget_level, Dict[query_uuid, scores]]]
    """
    all_aggregated = {}
    all_metrics = {}

    # Process each model
    for model in models:
        print(f"\n{'#'*80}")
        print(f"MODEL: {model}")
        print(f"{'#'*80}")

        # Get evaluation results for this model
        model_eval = evaluation_results.get(model) if evaluation_results else None

        # Aggregate metrics
        aggregated = aggregate_metrics(model, pass_number, budget_levels, model_eval)

        # Calculate key metrics
        metrics = calculate_key_metrics(aggregated)

        all_aggregated[model] = aggregated
        all_metrics[model] = metrics

    # Print summary table
    print_summary_table(all_metrics, all_aggregated)


def main():
    parser = argparse.ArgumentParser(
        description="Analyze budget constraint experiment results"
    )

    # Mode selection
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument("--evaluate", action="store_true",
                           help="Run CommonLLMJudge evaluation only")
    mode_group.add_argument("--analyze", action="store_true",
                           help="Analyze existing evaluation results")
    mode_group.add_argument("--full", action="store_true",
                           help="Full pipeline: evaluate + analyze")

    # Model configuration
    parser.add_argument("--model", type=str,
                       help="Single model to analyze")
    parser.add_argument("--models", type=str,
                       help="Comma-separated list of models to compare")

    # Experiment configuration
    parser.add_argument("--pass-number", type=int, default=1,
                       help="Pass number (default: 1)")
    parser.add_argument("--budget-levels", type=str,
                       default="budget_3,budget_5,budget_7,baseline",
                       help="Comma-separated budget levels (default: budget_3,budget_5,budget_7,baseline)")

    # Judge configuration
    parser.add_argument("--judges", type=str,
                       default=",".join(DEFAULT_JUDGES),
                       help=f"Comma-separated judge models (default: {','.join(DEFAULT_JUDGES)})")

    args = parser.parse_args()

    # Parse models
    if args.models:
        models = [m.strip() for m in args.models.split(",")]
    elif args.model:
        models = [args.model]
    else:
        parser.error("Either --model or --models must be specified")

    # Parse budget levels
    budget_levels = [b.strip() for b in args.budget_levels.split(",")]

    # Parse judges
    judge_models = [j.strip() for j in args.judges.split(",")]

    print("=" * 80)
    print("BUDGET CONSTRAINT EXPERIMENT ANALYSIS")
    print("=" * 80)
    print(f"Models:        {', '.join(models)}")
    print(f"Pass number:   {args.pass_number}")
    print(f"Budget levels: {', '.join(budget_levels)}")
    if args.evaluate or args.full:
        print(f"Judges:        {', '.join(judge_models)}")
    print("=" * 80)

    evaluation_results = {}

    # Step 1: Evaluation
    if args.evaluate or args.full:
        for model in models:
            print(f"\n{'#'*80}")
            print(f"EVALUATING MODEL: {model}")
            print(f"{'#'*80}")

            results = run_commonllm_judge(
                model=model,
                pass_number=args.pass_number,
                budget_levels=budget_levels,
                judge_models=judge_models
            )
            evaluation_results[model] = results

    # Step 2: Analysis
    if args.analyze or args.full:
        # Load existing evaluation scores if not already evaluated
        if not evaluation_results:
            for model in models:
                print(f"\n{'#'*80}")
                print(f"LOADING SCORES FOR MODEL: {model}")
                print(f"{'#'*80}")

                results = load_evaluation_scores(
                    model=model,
                    pass_number=args.pass_number,
                    budget_levels=budget_levels,
                    judge_models=judge_models
                )
                evaluation_results[model] = results

        analyze_budget_experiment(
            models=models,
            pass_number=args.pass_number,
            budget_levels=budget_levels,
            evaluation_results=evaluation_results if evaluation_results else None
        )


if __name__ == "__main__":
    main()
