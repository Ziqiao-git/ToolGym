#!/usr/bin/env python3
"""
concurrent_judge.py
å¹¶å‘è¯„æµ‹å¤šä¸ªtrajectoryæ–‡ä»¶

æ”¯æŒä¸¤ç§æ¨¡å¼:
1. æ‰¹é‡æ¨¡å¼: ä»Žprompt.jsonè¯»å–queriesï¼ŒåŒ¹é…traj_dirä¸­çš„æ‰€æœ‰trajectories
2. å•trajectoryæ¨¡å¼: è¯„æµ‹å•ä¸ªtrajectoryæ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
  # æ‰¹é‡å¹¶å‘è¯„æµ‹
  python runtime/concurrent_judge.py \
    --prompt prompt.json \
    --step-by-step \
    --traj_dir trajectories/anthropic-claude-3.5-sonnet/pass@1 \
    --model openai/gpt-4o-mini \
    --save_json evaluation/results.json \
    --workers 5

  # å•ä¸ªtrajectoryè¯„æµ‹
  python runtime/concurrent_judge.py \
    --trajectory trajectories/trajectory_20251114_140303.json \
    --model openai/gpt-4o-mini \
    --workers 1
"""

from __future__ import annotations
import os
import sys
import json
import argparse
import glob
from pathlib import Path
from typing import Any, Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# Import from commonllmjudge
sys.path.insert(0, str(Path(__file__).parent.parent / "Orchestrator"))
from mcpuniverse.evaluator.commonllmjudge import (
    load_prompts,
    load_all_trajectories,
    match_traj_by_query,
    match_traj_by_uuid,
    evaluate_trajectory_with_steps,
    llm_as_judge_score,
    _values_from_prompt_and_traj,
)


def evaluate_single_item(
    idx: int,
    total: int,
    query: str,
    query_uuid: Optional[str],
    traj_obj: Dict[str, Any],
    reference_tools: List[Dict[str, Any]],
    hard_constraints: List[Dict[str, Any]],
    soft_constraints: List[Dict[str, Any]],
    model_name: str,
    temperature: float,
    pass_threshold: float,
    step_by_step: bool,
) -> Dict[str, Any]:
    """
    è¯„æµ‹å•ä¸ªquery-trajectoryå¯¹

    Args:
        idx: å½“å‰ä»»åŠ¡ç´¢å¼•
        total: æ€»ä»»åŠ¡æ•°
        query: æŸ¥è¯¢æ–‡æœ¬
        query_uuid: æŸ¥è¯¢UUID
        traj_obj: trajectoryå¯¹è±¡
        reference_tools: å‚è€ƒå·¥å…·åˆ—è¡¨
        hard_constraints: ç¡¬çº¦æŸåˆ—è¡¨
        soft_constraints: è½¯çº¦æŸåˆ—è¡¨
        model_name: LLMæ¨¡åž‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        pass_threshold: é€šè¿‡é˜ˆå€¼
        step_by_step: æ˜¯å¦å¯ç”¨é€æ­¥è¯„æµ‹

    Returns:
        è¯„æµ‹ç»“æžœå­—å…¸
    """
    try:
        print(f"[{idx}/{total}] ðŸ”„ Evaluating: {query[:80]}...", file=sys.stderr, flush=True)

        if step_by_step:
            # é€æ­¥è¯„æµ‹æ¨¡å¼
            result = evaluate_trajectory_with_steps(
                traj_obj=traj_obj,
                query=query,
                reference_tools=reference_tools,
                hard_constraints=hard_constraints,
                soft_constraints=soft_constraints,
                query_uuid=query_uuid,
                model_name=model_name,
                temperature=temperature,
                pass_threshold=pass_threshold,
            )
        else:
            # æ•´ä½“è¯„æµ‹æ¨¡å¼
            task_id = traj_obj.get("_filename", f"task_{idx}")
            pack = _values_from_prompt_and_traj(
                query, traj_obj, task_id, reference_tools,
                hard_constraints, soft_constraints
            )

            eval_obj = llm_as_judge_score(
                meta=pack["meta"],
                task=pack["task"],
                history=pack["history"],
                final_answer=pack["final_answer"],
                reference_tools=pack["reference_tools"],
                hard_constraints=pack["hard_constraints"],
                soft_constraints=pack["soft_constraints"],
                temperature=temperature,
                pass_threshold=pass_threshold,
                model_name=model_name,
            )

            result = {
                "task_id": task_id,
                "uuid": query_uuid,
                "query": query,
                "reference_tools": reference_tools,
                "hard_constraints": hard_constraints,
                "soft_constraints": soft_constraints,
                "actual_tools": pack.get("actual_tools", []),
                "binary": eval_obj.get("binary"),
                "score": eval_obj.get("score"),
                "answer_reasonableness": eval_obj.get("answer_reasonableness"),
                "tool_correctness": eval_obj.get("tool_correctness"),
                "tool_relevance": eval_obj.get("tool_relevance"),
                "grounding": eval_obj.get("grounding"),
                "constraint_adherence": eval_obj.get("constraint_adherence"),
                "reasons": eval_obj.get("reasons", {}),
                "explanation": eval_obj.get("explanation"),
            }

        status = "âœ…" if result.get("binary") == "success" or \
                       result.get("holistic_evaluation", {}).get("binary") == "success" else "âŒ"
        print(f"[{idx}/{total}] {status} Completed: {query[:80]}...", file=sys.stderr, flush=True)

        return result

    except Exception as e:
        print(f"[{idx}/{total}] âŒ Error evaluating query: {str(e)}", file=sys.stderr, flush=True)
        return {
            "task_id": f"prompt_idx_{idx}",
            "uuid": query_uuid,
            "query": query,
            "error": str(e),
        }


def concurrent_batch_evaluation(
    prompt_path: str,
    traj_dir: str,
    model_name: str,
    temperature: float,
    pass_threshold: float,
    step_by_step: bool,
    max_workers: int = 5,
) -> List[Dict[str, Any]]:
    """
    å¹¶å‘æ‰¹é‡è¯„æµ‹

    Args:
        prompt_path: prompt.jsonæ–‡ä»¶è·¯å¾„
        traj_dir: trajectoriesç›®å½•è·¯å¾„
        model_name: LLMæ¨¡åž‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        pass_threshold: é€šè¿‡é˜ˆå€¼
        step_by_step: æ˜¯å¦å¯ç”¨é€æ­¥è¯„æµ‹
        max_workers: æœ€å¤§å¹¶å‘workeræ•°é‡

    Returns:
        è¯„æµ‹ç»“æžœåˆ—è¡¨
    """
    print(f"\n{'='*70}")
    print(f"ðŸš€ Concurrent Batch Evaluation")
    print(f"{'='*70}")
    print(f"Prompt file: {prompt_path}")
    print(f"Trajectories dir: {traj_dir}")
    print(f"Model: {model_name}")
    print(f"Max workers: {max_workers}")
    print(f"Step-by-step: {step_by_step}")
    print(f"{'='*70}\n")

    # åŠ è½½æ•°æ®
    print("ðŸ“‚ Loading prompts and trajectories...", file=sys.stderr)
    items = load_prompts(prompt_path)
    trajs = load_all_trajectories(traj_dir)
    print(f"   Found {len(items)} queries and {len(trajs)} trajectories", file=sys.stderr)

    # åŒ¹é…queryå’Œtrajectory
    tasks = []
    for idx, item in enumerate(items, 1):
        query_uuid = item.get("uuid")
        query = item.get("query", "")
        ref_tools = item.get("reference_tools", []) or []
        hard_constraints = item.get("hard_constraints", []) or []
        soft_constraints = item.get("soft_constraints", []) or []

        # ä¼˜å…ˆä½¿ç”¨UUIDåŒ¹é…ï¼Œfallbackåˆ°queryæ–‡æœ¬åŒ¹é…
        matched = None
        if query_uuid:
            matched = match_traj_by_uuid(trajs, query_uuid)
        if not matched:
            matched = match_traj_by_query(trajs, query)

        if not matched:
            print(f"âš ï¸  [{idx}/{len(items)}] No trajectory found for: {query[:80]}...", file=sys.stderr)
            tasks.append({
                "idx": idx,
                "result": {
                    "task_id": f"prompt_idx_{idx}",
                    "uuid": query_uuid,
                    "query": query,
                    "error": "No trajectory matched for this query.",
                },
                "matched": False,
            })
        else:
            tasks.append({
                "idx": idx,
                "query": query,
                "query_uuid": query_uuid,
                "traj_obj": matched,
                "reference_tools": ref_tools,
                "hard_constraints": hard_constraints,
                "soft_constraints": soft_constraints,
                "matched": True,
            })

    # å¹¶å‘æ‰§è¡Œè¯„æµ‹
    results = []
    total_tasks = len(items)
    matched_tasks = [t for t in tasks if t.get("matched", False)]

    print(f"\nâš¡ Starting concurrent evaluation of {len(matched_tasks)} matched trajectories...\n", file=sys.stderr)
    start_time = datetime.now()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {}
        for task in tasks:
            if task.get("matched"):
                future = executor.submit(
                    evaluate_single_item,
                    idx=task["idx"],
                    total=total_tasks,
                    query=task["query"],
                    query_uuid=task.get("query_uuid"),
                    traj_obj=task["traj_obj"],
                    reference_tools=task["reference_tools"],
                    hard_constraints=task["hard_constraints"],
                    soft_constraints=task["soft_constraints"],
                    model_name=model_name,
                    temperature=temperature,
                    pass_threshold=pass_threshold,
                    step_by_step=step_by_step,
                )
                future_to_task[future] = task
            else:
                # æœªåŒ¹é…çš„ä»»åŠ¡ç›´æŽ¥æ·»åŠ ç»“æžœ
                results.append(task["result"])

        # æ”¶é›†ç»“æžœ
        completed = 0
        for future in as_completed(future_to_task):
            completed += 1
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                task = future_to_task[future]
                print(f"âŒ Task {task['idx']} raised exception: {e}", file=sys.stderr)
                results.append({
                    "task_id": f"prompt_idx_{task['idx']}",
                    "uuid": task.get("query_uuid"),
                    "query": task.get("query", ""),
                    "error": f"Exception during evaluation: {str(e)}",
                })

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print(f"\n{'='*70}")
    print(f"âœ… Concurrent evaluation complete!")
    print(f"   Total tasks: {total_tasks}")
    print(f"   Matched: {len(matched_tasks)}")
    print(f"   Duration: {duration:.2f}s")
    print(f"   Average: {duration/len(matched_tasks):.2f}s per task")
    print(f"{'='*70}\n")

    # æŒ‰ç…§åŽŸå§‹é¡ºåºæŽ’åºç»“æžœ
    results.sort(key=lambda x: int(x.get("task_id", "prompt_idx_0").split("_")[-1]) if "prompt_idx" in x.get("task_id", "") else 0)

    return results


def evaluate_single_trajectory(
    trajectory_path: str,
    model_name: str,
    temperature: float,
    pass_threshold: float,
    step_by_step: bool,
) -> List[Dict[str, Any]]:
    """
    è¯„æµ‹å•ä¸ªtrajectoryæ–‡ä»¶

    Args:
        trajectory_path: trajectoryæ–‡ä»¶è·¯å¾„
        model_name: LLMæ¨¡åž‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        pass_threshold: é€šè¿‡é˜ˆå€¼
        step_by_step: æ˜¯å¦å¯ç”¨é€æ­¥è¯„æµ‹

    Returns:
        è¯„æµ‹ç»“æžœåˆ—è¡¨ï¼ˆå•ä¸ªå…ƒç´ ï¼‰
    """
    print(f"\n{'='*70}")
    print(f"ðŸ“ Single Trajectory Evaluation")
    print(f"{'='*70}")
    print(f"Trajectory: {trajectory_path}")
    print(f"Model: {model_name}")
    print(f"Step-by-step: {step_by_step}")
    print(f"{'='*70}\n")

    traj_path = Path(trajectory_path)
    if not traj_path.exists():
        print(f"âŒ Trajectory file not found: {traj_path}", file=sys.stderr)
        sys.exit(1)

    with open(traj_path, 'r', encoding='utf-8') as f:
        traj_data = json.load(f)

    query = traj_data.get("metadata", {}).get("query", "")
    query_uuid = traj_data.get("metadata", {}).get("query_uuid")

    if not query:
        print("âŒ No query found in trajectory metadata", file=sys.stderr)
        sys.exit(1)

    print(f"Query: {query}\n", file=sys.stderr)

    result = evaluate_single_item(
        idx=1,
        total=1,
        query=query,
        query_uuid=query_uuid,
        traj_obj=traj_data,
        reference_tools=[],
        hard_constraints=[],
        soft_constraints=[],
        model_name=model_name,
        temperature=temperature,
        pass_threshold=pass_threshold,
        step_by_step=step_by_step,
    )

    return [result]


def main():
    parser = argparse.ArgumentParser(
        description="å¹¶å‘è¯„æµ‹å¤šä¸ªtrajectoryæ–‡ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æ‰¹é‡å¹¶å‘è¯„æµ‹ï¼ˆ5ä¸ªworkerï¼‰
  python runtime/concurrent_judge.py \\
    --prompt prompt.json \\
    --traj_dir trajectories/anthropic-claude-3.5-sonnet/pass@1 \\
    --model openai/gpt-4o-mini \\
    --workers 5 \\
    --save_json evaluation/results.json

  # å•ä¸ªtrajectoryè¯„æµ‹
  python runtime/concurrent_judge.py \\
    --trajectory trajectories/trajectory_20251114_140303.json \\
    --model openai/gpt-4o-mini \\
    --step-by-step
        """
    )

    # è¾“å…¥å‚æ•°
    parser.add_argument("--prompt", default="prompt.json",
                       help="è·¯å¾„åˆ°NEWæ ¼å¼çš„prompt.jsonæ–‡ä»¶ (æ‰¹é‡æ¨¡å¼)")
    parser.add_argument("--traj_dir", default="trajectories",
                       help="trajectoriesç›®å½•è·¯å¾„ (æ‰¹é‡æ¨¡å¼)")
    parser.add_argument("--trajectory", default="",
                       help="å•ä¸ªtrajectoryæ–‡ä»¶è·¯å¾„ (å•æ–‡ä»¶æ¨¡å¼)")

    # è¯„æµ‹å‚æ•°
    parser.add_argument("--threshold", type=float, default=0.85,
                       help="é€šè¿‡é˜ˆå€¼ (é»˜è®¤: 0.85)")
    parser.add_argument("--temperature", type=float, default=0.0,
                       help="LLMæ¸©åº¦å‚æ•° (é»˜è®¤: 0.0)")
    parser.add_argument("--model", default="openai/gpt-4o-mini",
                       help="LLMæ¨¡åž‹åç§° (é»˜è®¤: openai/gpt-4o-mini)")
    parser.add_argument("--step-by-step", action="store_true",
                       help="å¯ç”¨é€æ­¥è¯„æµ‹æ¨¡å¼")

    # å¹¶å‘å‚æ•°
    parser.add_argument("--workers", type=int, default=5,
                       help="æœ€å¤§å¹¶å‘workeræ•°é‡ (é»˜è®¤: 5)")

    # è¾“å‡ºå‚æ•°
    parser.add_argument("--save_json", default="",
                       help="ä¿å­˜JSONç»“æžœçš„è·¯å¾„")

    args = parser.parse_args()

    # æ£€æŸ¥çŽ¯å¢ƒå˜é‡
    if not os.getenv("OPENAI_API_KEY") or not os.getenv("OPENAI_BASE_URL"):
        print("âŒ Error: Please set OPENAI_API_KEY and OPENAI_BASE_URL environment variables.",
              file=sys.stderr)
        sys.exit(1)

    # é€‰æ‹©æ¨¡å¼
    if args.trajectory:
        # å•æ–‡ä»¶æ¨¡å¼
        results = evaluate_single_trajectory(
            trajectory_path=args.trajectory,
            model_name=args.model,
            temperature=args.temperature,
            pass_threshold=args.threshold,
            step_by_step=args.step_by_step,
        )
    else:
        # æ‰¹é‡æ¨¡å¼
        results = concurrent_batch_evaluation(
            prompt_path=args.prompt,
            traj_dir=args.traj_dir,
            model_name=args.model,
            temperature=args.temperature,
            pass_threshold=args.threshold,
            step_by_step=args.step_by_step,
            max_workers=args.workers,
        )

    # è¾“å‡ºç»“æžœ
    out = json.dumps(results, ensure_ascii=False, indent=2)
    print(out)

    # ä¿å­˜ç»“æžœ
    save_path = args.save_json
    if not save_path and args.trajectory and args.step_by_step:
        # è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
        import re
        traj_name = Path(args.trajectory).stem
        match = re.search(r'(\d{8}_\d{6})', traj_name)
        if match:
            timestamp = match.group(1)
            save_path = f"evaluation/concurrent_step_{timestamp}.json"
        else:
            save_path = "evaluation/concurrent_results.json"
        Path("evaluation").mkdir(exist_ok=True)

    if save_path:
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(out)
        print(f"\nðŸ’¾ Saved results to {save_path}", file=sys.stderr)

    # ç»Ÿè®¡æ‘˜è¦
    if results:
        total = len(results)
        errors = sum(1 for r in results if "error" in r)

        # è®¡ç®—æˆåŠŸçŽ‡ï¼ˆå¤„ç†ä¸åŒçš„ç»“æžœæ ¼å¼ï¼‰
        successes = 0
        for r in results:
            if "error" not in r:
                # æ£€æŸ¥holistic_evaluationï¼ˆstep-by-stepæ¨¡å¼ï¼‰
                if "holistic_evaluation" in r:
                    if r.get("holistic_evaluation", {}).get("binary") == "success":
                        successes += 1
                # æ£€æŸ¥binaryï¼ˆæ™®é€šæ¨¡å¼ï¼‰
                elif r.get("binary") == "success":
                    successes += 1

        print(f"\nðŸ“Š Summary:", file=sys.stderr)
        print(f"   Total: {total}", file=sys.stderr)
        print(f"   Success: {successes}/{total-errors} ({successes/(total-errors)*100:.1f}%)" if total > errors else "   Success: 0/0 (N/A)", file=sys.stderr)
        print(f"   Errors: {errors}", file=sys.stderr)


if __name__ == "__main__":
    main()
